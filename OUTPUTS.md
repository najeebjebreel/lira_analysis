# LiRA Attack Outputs and Data Format Reference

This document describes all outputs generated by the LiRA attack implementation, including likelihood ratios, scores, and intermediate results.

## Table of Contents

- [Directory Structure](#directory-structure)
- [Training Outputs](#training-outputs)
- [Attack Outputs](#attack-outputs)
- [Score Files](#score-files)
- [Analysis Outputs](#analysis-outputs)
- [Data Formats](#data-formats)
- [Loading and Using Outputs](#loading-and-using-outputs)

---

## Directory Structure

After running training and attacks, your experiment directory will have this structure:

```
experiments/{dataset}/{model}/{timestamp}/
├── train_config.yaml                 # Saved training configuration
├── attack_config.yaml                # Saved attack configuration
├── train_log.log                     # Training logs
├── attack_log.log                    # Attack logs
├── keep_indices.npy                  # [M, N] boolean: training membership
│
├── model_0/                          # First shadow model
│   ├── best_model.pth                    # Best checkpoint (lowest val loss)
│   ├── checkpoint_epochN.pth             # Per-epoch checkpoints
│   ├── logits/
│   │   └── logits.npy                    # [N, 1, A, C] model outputs
│   └── scores/
│       └── scores.npy                    # [N] membership scores
│
├── model_1/, model_2/, ..., model_M-1/   # Additional shadow models
│
├── roc_curve_single.pdf              # ROC curves (single-target mode)
├── attack_results_single.csv         # Metrics table (single-target)
├── train_test_stats.csv              # Training/test loss and accuracy
│
├── attack_results_leave_one_out_summary.csv  # Aggregated LOO results
├── membership_labels.npy             # [M, N] ground truth labels
├── online_scores_leave_one_out.npy   # [M, N] LiRA online scores
├── online_fixed_scores_leave_one_out.npy    # [M, N] LiRA online (fixed var)
├── offline_scores_leave_one_out.npy  # [M, N] LiRA offline scores
├── offline_fixed_scores_leave_one_out.npy   # [M, N] LiRA offline (fixed var)
├── global_scores_leave_one_out.npy   # [M, N] Global threshold scores
└── threshold_info_leave_one_out.csv  # Per-target threshold analysis
```

---

## Training Outputs

### `keep_indices.npy`

**Shape:** `[M, N]` where M = number of shadow models, N = dataset size

**Type:** `bool` (numpy boolean array)

**Description:** Training membership matrix. `keep_indices[i, j] = True` means sample `j` was in the training set of shadow model `i`.

**Example:**
```python
import numpy as np
keep_indices = np.load('experiments/.../keep_indices.npy')
print(keep_indices.shape)  # (256, 60000) for 256 models on CIFAR-10
print(keep_indices[0].sum())  # ~30000 (about 50% of data)
```

### `train_config.yaml`

**Format:** YAML

**Description:** Complete training configuration including:
- Dataset settings
- Model architecture
- Training hyperparameters (epochs, batch size, optimizer, learning rate)
- Data augmentations
- Random seed

**Example:**
```yaml
dataset:
  name: cifar10
  num_classes: 10
training:
  num_shadow_models: 256
  epochs: 100
  batch_size: 256
model:
  architecture: resnet18
seed: 42
```

### `model_i/best_model.pth`

**Format:** PyTorch checkpoint dictionary

**Description:** Saved model with lowest validation loss. Contains:
- `state_dict` or `model_state_dict`: Model weights
- Optionally: `optimizer_state_dict`, `epoch`, `loss`, etc.

**Loading:**
```python
import torch
checkpoint = torch.load('model_0/best_model.pth')
model.load_state_dict(checkpoint.get('state_dict', checkpoint))
```

---

## Attack Outputs

### `model_i/logits/logits.npy`

**Shape:** `[N, 1, A, C]` where:
- N = number of samples
- 1 = single model dimension (for compatibility)
- A = number of augmentations per sample
- C = number of classes

**Type:** `float32`

**Description:** Raw model outputs (logits) before softmax. Generated with data augmentations for robustness.

**Augmentation count depends on config:**
- No augmentation: A = 1
- Horizontal flip only: A = 2
- Spatial shift (s=2) + flip: A = 2 × (2×2+1) = 10
- Spatial shift (s=4, stride=2) + flip: A = 2 × (5×5) = 50

**Example:**
```python
logits = np.load('model_0/logits/logits.npy')
print(logits.shape)  # (60000, 1, 10, 10) for CIFAR-10 with augmentations
# Extract original (non-augmented) logits
original_logits = logits[:, 0, 0, :]  # [N, C]
```

### `model_i/scores/scores.npy`

**Shape:** `[N]`

**Type:** `float64`

**Description:** Per-sample membership scores computed as:
```
score = log(P_true_class) - log(P_other_classes)
```

Where probabilities are averaged over augmentations. **Higher scores indicate higher likelihood of membership.**

**Example:**
```python
scores = np.load('model_0/scores/scores.npy')
print(scores.shape)  # (60000,)
print(scores.mean(), scores.std())  # Mean and std of scores
```

---

## Score Files (Leave-One-Out Mode)

These files are only generated when `attack.evaluation_mode` includes `leave_one_out`.

### `membership_labels.npy`

**Shape:** `[M, N]`

**Type:** `bool`

**Description:** Ground truth membership labels for leave-one-out evaluation. `membership_labels[i, j] = True` means sample `j` was in training set of model `i` (same as `keep_indices`).

### Attack Score Files

All attack score files have the same format:

**Shape:** `[M, N]`

**Type:** `float64`

**Description:** Attack scores where **higher values indicate higher confidence that the sample was a member**.

**Files:**
1. **`online_scores_leave_one_out.npy`**: LiRA online (uses both in/out distribution)
2. **`online_fixed_scores_leave_one_out.npy`**: LiRA online with fixed variance
3. **`offline_scores_leave_one_out.npy`**: LiRA offline (uses only out distribution)
4. **`offline_fixed_scores_leave_one_out.npy`**: LiRA offline with fixed variance
5. **`global_scores_leave_one_out.npy`**: Global threshold baseline

**Example:**
```python
import numpy as np

# Load attack scores and labels
labels = np.load('membership_labels.npy')
online_scores = np.load('online_scores_leave_one_out.npy')
offline_scores = np.load('offline_scores_leave_one_out.npy')

print(online_scores.shape)  # (256, 60000)

# Analyze scores for first target model
target_idx = 0
true_members = labels[target_idx]
member_scores = online_scores[target_idx][true_members]
non_member_scores = online_scores[target_idx][~true_members]

print(f"Member scores: {member_scores.mean():.3f} ± {member_scores.std():.3f}")
print(f"Non-member scores: {non_member_scores.mean():.3f} ± {non_member_scores.std():.3f}")
```

---

## Analysis Outputs

### `attack_results_single.csv`

**Format:** CSV with columns:
- `Attack`: Attack variant name
- `Acc`: Maximum accuracy (%)
- `AUC`: Area under ROC curve (%)
- `TPR@X%FPR`: True positive rate at X% false positive rate
- `Prec@X%FPR`: Precision at X% false positive rate

**Example:**
```python
import pandas as pd
df = pd.read_csv('attack_results_single.csv')
print(df[['Attack', 'AUC', 'TPR@0.0010%FPR']])
```

### `attack_results_leave_one_out_summary.csv`

**Format:** CSV with columns:
- `Attack`: Attack variant name
- `AUC Mean`, `AUC Std`: Mean ± std AUC across targets
- `Acc Mean`, `Acc Std`: Mean ± std accuracy
- `TPR@X%FPR Mean`, `TPR@X%FPR Std`: Mean ± std TPR at various FPRs
- `Prec@X%FPR Mean`, `Prec@X%FPR Std`: Mean ± std precision

**Description:** Aggregated statistics from leave-one-out evaluation with uncertainty quantification.

### `threshold_info_leave_one_out.csv`

**Format:** CSV with columns:
- `attack`: Attack variant
- `target_model`: Model index used as target
- `target_fpr`: Target FPR threshold
- `actual_fpr`: Achieved FPR
- `threshold`: Decision threshold value
- `tpr`: True positive rate achieved
- `precision`: Precision achieved

**Description:** Per-target, per-attack threshold information for detailed analysis.

**Example:**
```python
df = pd.read_csv('threshold_info_leave_one_out.csv')
# Analyze threshold stability for online attack
online = df[df['attack'] == 'LiRA (online)']
print(f"Threshold range: {online['threshold'].min():.3f} - {online['threshold'].max():.3f}")
print(f"Threshold std: {online['threshold'].std():.3f}")
```

### `train_test_stats.csv`

**Format:** CSV with columns:
- `Set`: 'Train' or 'Test'
- `Loss Mean`, `Loss STD`: Cross-entropy loss statistics
- `Acc (%) Mean`, `Acc (%) STD`: Accuracy statistics

**Description:** Training and test set statistics computed from original (non-augmented) logits across all shadow models.

---

## Data Formats

### Numpy Array Conventions

- **Boolean masks**: Use `dtype=bool` for memory efficiency
- **Scores/probabilities**: Use `dtype=float64` for numerical precision
- **Logits**: Use `dtype=float32` for storage efficiency
- **Indices**: Use `dtype=int32` or `int64`

### Shape Conventions

- **[M, N]**: M models, N samples (e.g., scores, labels)
- **[N, C]**: N samples, C classes (e.g., logits)
- **[N, A, C]**: N samples, A augmentations, C classes
- **[N, 1, A, C]**: Added dimension for compatibility

### Score Semantics

**Important:** All score files use the convention **"higher = more likely member"**

- LiRA scores are already in this format (likelihood ratios)
- Internal attack functions may use "lower = member" but outputs are flipped
- When computing ROC curves, use `-score` for compatibility with sklearn

---

## Loading and Using Outputs

### Complete Analysis Example

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Load experiment results
exp_dir = "experiments/cifar10/resnet18/2024-01-01_0000"

# Load leave-one-out results
labels = np.load(f"{exp_dir}/membership_labels.npy")
online_scores = np.load(f"{exp_dir}/online_scores_leave_one_out.npy")
summary = pd.read_csv(f"{exp_dir}/attack_results_leave_one_out_summary.csv")

# Analyze first target model
target_idx = 0
y_true = labels[target_idx]
y_score = online_scores[target_idx]

# Compute ROC curve
fpr, tpr, thresholds = roc_curve(y_true, y_score)
roc_auc = auc(fpr, tpr)

# Plot
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.3f}')
plt.xscale('log')
plt.yscale('log')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title(f'ROC Curve - Target Model {target_idx}')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig(f'{exp_dir}/custom_roc_model_{target_idx}.pdf')
plt.show()

# Find TPR at specific FPR
target_fpr = 0.001  # 0.1% FPR
idx = np.where(fpr <= target_fpr)[0]
if len(idx) > 0:
    tpr_at_fpr = tpr[idx[-1]]
    print(f"TPR @ {target_fpr*100}% FPR: {tpr_at_fpr*100:.2f}%")
```

### Per-Sample Vulnerability Analysis

```python
# Identify most vulnerable samples (frequently detected when members)
M, N = labels.shape

# Count true positives per sample across all models
tp_counts = np.zeros(N, dtype=int)
fp_counts = np.zeros(N, dtype=int)

threshold = 0.0  # Adjust based on desired FPR
for i in range(M):
    predictions = online_scores[i] >= threshold
    tp_counts += predictions & labels[i]
    fp_counts += predictions & ~labels[i]

# Most vulnerable: high TP, low FP
vulnerability_score = tp_counts - fp_counts
most_vulnerable_idx = np.argsort(vulnerability_score)[::-1][:100]

print(f"Most vulnerable sample: idx={most_vulnerable_idx[0]}")
print(f"  TP count: {tp_counts[most_vulnerable_idx[0]]}")
print(f"  FP count: {fp_counts[most_vulnerable_idx[0]]}")
```

### Comparing Attack Variants

```python
# Load all attack scores
attacks = {
    'Online': np.load(f'{exp_dir}/online_scores_leave_one_out.npy'),
    'Offline': np.load(f'{exp_dir}/offline_scores_leave_one_out.npy'),
    'Global': np.load(f'{exp_dir}/global_scores_leave_one_out.npy'),
}

# Compute agreement between attacks
target_idx = 0
threshold = 0.0

for name1, scores1 in attacks.items():
    for name2, scores2 in attacks.items():
        if name1 >= name2:  # Avoid duplicates
            continue
        pred1 = scores1[target_idx] >= threshold
        pred2 = scores2[target_idx] >= threshold
        agreement = np.mean(pred1 == pred2)
        print(f"{name1} vs {name2}: {agreement*100:.1f}% agreement")
```

---

## Advanced Usage: Post-Analysis

The `post_analysis.ipynb` notebook provides advanced analysis including:

1. **Per-model metrics with two threshold modes:**
   - Target mode: Threshold computed from target model's ROC
   - Shadow mode: Threshold = median of other models' target thresholds

2. **Per-sample vulnerability ranking:**
   - Computes TP/FP/TN/FN per sample across leave-one-out models
   - Identifies samples reliably detected when members, rarely flagged when non-members

3. **Precision at different priors:**
   - Computes precision assuming different membership priors (1%, 10%, 50%)
   - Important for real-world threat modeling

4. **LaTeX table generation:**
   - Creates publication-ready tables with reduction factors
   - Compares multiple benchmarks side-by-side

5. **Vulnerable sample visualization:**
   - Grid visualization of most vulnerable samples
   - Annotated with TP/FP counts

**Usage:**
```python
# See post_analysis.ipynb for complete implementation
# Key outputs:
# - per_model_metrics_two_modes.csv
# - summary_statistics_two_modes.csv
# - samples_vulnerability_ranked_online_shadow_0p001pct.csv
# - samples_highly_vulnerable_online_shadow_0p001pct.csv
# - top20_vulnerable_online_shadow_0p001pct.png
```

---

## Tips and Best Practices

### Storage Considerations

- **Logits files are large**: [N, 1, A, C] × float32 × M models
  - CIFAR-10 (60K samples, 10 augs, 10 classes, 256 models): ~60 GB
  - Consider deleting logits after computing scores to save space

- **Score files are compact**: [N] × float64 × M models
  - CIFAR-10 (60K samples, 256 models): ~120 MB

### Loading Large Files

For large experiments, use memory mapping:

```python
# Memory-mapped loading (doesn't load entire file into RAM)
logits = np.load('model_0/logits/logits.npy', mmap_mode='r')
# Access specific samples without loading all
sample_0_logits = logits[0]  # Only loads this sample
```

### Cleaning Up

To save space after attack completion:

```bash
# Delete logits (can be regenerated if needed)
find experiments/ -name "logits.npy" -delete

# Keep only scores and final results
# Total space: ~200 MB for 256 models on CIFAR-10
```

---

## Troubleshooting

### File Not Found

**Problem:** `FileNotFoundError: membership_labels.npy`

**Solution:** Run attack with `evaluation_mode=leave_one_out` or `both`:
```bash
python attack.py --config configs/config_attack.yaml \
    --override attack.evaluation_mode=leave_one_out
```

### Shape Mismatch

**Problem:** `ValueError: shapes (256, 60000) and (255, 60000) don't match`

**Solution:** Some shadow models may have failed. Check logs and ensure all models completed training.

### Memory Error

**Problem:** `MemoryError` when loading large arrays

**Solution:** Use memory mapping or process models incrementally:
```python
# Process one model at a time
for i in range(num_models):
    scores = np.load(f'model_{i}/scores/scores.npy')
    # Process scores
    del scores  # Free memory
```

---

## Reference: Complete File List

| File | Size (CIFAR-10, 256 models) | Required | Description |
|------|-----|----------|-------------|
| `keep_indices.npy` | 2 MB | ✅ Yes | Training membership |
| `train_config.yaml` | <1 KB | ✅ Yes | Training config |
| `attack_config.yaml` | <1 KB | ✅ Yes | Attack config |
| `model_i/best_model.pth` | ~45 MB × 256 | ✅ Yes | Model checkpoints |
| `model_i/logits/logits.npy` | ~230 MB × 256 | ❌ No | Can regenerate |
| `model_i/scores/scores.npy` | ~500 KB × 256 | ✅ Yes | Membership scores |
| `membership_labels.npy` | 2 MB | ✅ Yes | Ground truth |
| `*_scores_leave_one_out.npy` | ~120 MB × 5 | ✅ Yes | Attack scores |
| `*.csv` | <1 MB total | ✅ Yes | Metrics tables |
| `*.pdf` | <1 MB total | ⚠️ Optional | Visualizations |

**Total essential storage:** ~13 GB (with logits: ~73 GB)

---

## Contributing

To add new output formats or metrics:

1. Update this documentation
2. Follow naming conventions (`*_leave_one_out.npy` for LOO files)
3. Use consistent data types and shapes
4. Add loading examples
5. Update the reference table

---

**Last Updated:** 2024
**Version:** 1.0
