# LiRA Attack Outputs and Data Format Reference

This document describes all outputs generated by the LiRA attack implementation, including likelihood ratios, scores, and intermediate results.

## Table of Contents

- [Directory Structure](#directory-structure)
- [Training Outputs](#training-outputs)
- [Attack Outputs](#attack-outputs)
- [Score Files](#score-files)
- [Analysis Outputs](#analysis-outputs)

---

## Directory Structure

After running training and attacks, your experiment directory will have this structure:

```
experiments/{dataset}/{model}/{timestamp}/
├── train_config.yaml                 # Saved training configuration
├── attack_config.yaml                # Saved attack configuration
├── train_log.log                     # Training logs
├── attack_log.log                    # Attack logs
├── keep_indices.npy                  # [M, N] boolean: training membership
│
├── model_0/                          # First shadow model
│   ├── best_model.pth                    # Best checkpoint (lowest val loss)
│   ├── checkpoint_epochN.pth             # Per-epoch checkpoints
│   ├── logits/
│   │   └── logits.npy                    # [N, 1, A, C] model outputs (A is number of augmentations, C is number of classes)
│   └── scores/
│       └── scores.npy                    # [N, A] membership scores
│
├── model_1/, model_2/, ..., model_M-1/   # Additional shadow models
│
├── roc_curve_single.pdf              # ROC curves (single-target mode)
├── attack_results_single.csv         # Metrics table (single-target)
├── train_test_stats.csv              # Training/test loss and accuracy
│
├── attack_results_leave_one_out_summary.csv  # Aggregated LOO results
├── membership_labels.npy             # [M, N] ground truth labels
├── online_scores_leave_one_out.npy   # [M, N] LiRA online scores
├── online_fixed_scores_leave_one_out.npy    # [M, N] LiRA online (fixed var)
├── offline_scores_leave_one_out.npy  # [M, N] LiRA offline scores
├── offline_fixed_scores_leave_one_out.npy   # [M, N] LiRA offline (fixed var)
├── global_scores_leave_one_out.npy   # [M, N] Global threshold scores
└── threshold_info_leave_one_out.csv  # Per-target threshold analysis
```

---

## Training Outputs

### `keep_indices.npy`

**Shape:** `[M, N]` where M = number of shadow models, N = dataset size

**Description:** Training membership matrix. `keep_indices[i, j] = True` means sample `j` was in the training set of shadow model `i`.


### `train_config.yaml`

**Description:** Complete training configuration including:
- Dataset settings
- Model architecture
- Training hyperparameters (epochs, batch size, optimizer, learning rate)
- Data augmentations
- Random seed

### `model_i/best_model.pth`

**Format:** PyTorch checkpoint dictionary

**Description:** Saved model with lowest validation loss. Contains:
- `state_dict` or `model_state_dict`: Model weights
- Optionally: `optimizer_state_dict`, `epoch`, `loss`, etc.


---

## Attack Outputs

### `model_i/logits/logits.npy`

**Shape:** `[N, 1, A, C]` where:
- N = number of samples
- 1 = single model dimension (for compatibility)
- A = number of augmentations per sample
- C = number of classes

**Description:** Raw model outputs (logits) before softmax. Generated with data augmentations for robustness.


### `model_i/scores/scores.npy`

**Shape:** `[N, A]`

**Description:** Per-sample membership scores computed as:
```
score = log(P_true_class) - log(P_other_classes)
```

---

## Score Files (Leave-One-Out Mode)

These files are only generated when `attack.evaluation_mode` includes `leave_one_out`.

### `membership_labels.npy`

**Shape:** `[M, N]`

**Type:** `bool`

**Description:** Ground truth membership labels for leave-one-out evaluation. `membership_labels[i, j] = True` means sample `j` was in training set of model `i` (same as `keep_indices`).

### Attack Score Files

All attack score files have the same format:

**Shape:** `[M, N]`

**Type:** `float64`

**Description:** Attack scores where **higher values indicate higher confidence that the sample was a member**.

**Files:**
1. **`online_scores_leave_one_out.npy`**: LiRA online (uses both in/out distribution)
2. **`online_fixed_scores_leave_one_out.npy`**: LiRA online with fixed variance
3. **`offline_scores_leave_one_out.npy`**: LiRA offline (uses only out distribution)
4. **`offline_fixed_scores_leave_one_out.npy`**: LiRA offline with fixed variance
5. **`global_scores_leave_one_out.npy`**: Global threshold baseline


---

## Analysis Outputs

### `attack_results_single.csv`

**Format:** CSV with columns:
- `Attack`: Attack variant name
- `Acc`: Maximum accuracy (%)
- `AUC`: Area under ROC curve (%)
- `TPR@X%FPR`: True positive rate at X% false positive rate
- `Prec@X%FPR`: Precision at X% false positive rate

### `attack_results_leave_one_out_summary.csv`

**Format:** CSV with columns:
- `Attack`: Attack variant name
- `AUC Mean`, `AUC Std`: Mean ± std AUC across targets
- `Acc Mean`, `Acc Std`: Mean ± std accuracy
- `TPR@X%FPR Mean`, `TPR@X%FPR Std`: Mean ± std TPR at various FPRs
- `Prec@X%FPR Mean`, `Prec@X%FPR Std`: Mean ± std precision

**Description:** Aggregated statistics from leave-one-out evaluation with uncertainty quantification.

### `threshold_info_leave_one_out.csv`

**Format:** CSV with columns:
- `attack`: Attack variant
- `target_model`: Model index used as target
- `target_fpr`: Target FPR threshold
- `actual_fpr`: Achieved FPR
- `threshold`: Decision threshold value
- `tpr`: True positive rate achieved
- `precision`: Precision achieved

**Description:** Per-target, per-attack threshold information for detailed analysis.


### `train_test_stats.csv`

**Format:** CSV with columns:
- `Set`: 'Train' or 'Test'
- `Loss Mean`, `Loss STD`: Cross-entropy loss statistics
- `Acc (%) Mean`, `Acc (%) STD`: Accuracy statistics

**Description:** Training and test set statistics computed from original (non-augmented) logits across all shadow models.

---

### Score Semantics

**Important:** All score files use the convention **"higher = more likely member"**

---





