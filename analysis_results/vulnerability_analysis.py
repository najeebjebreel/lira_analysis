"""
Per-sample vulnerability analysis for LiRA attacks.

This script identifies and ranks samples by their vulnerability to
membership inference attacks.

Usage:
    python vulnerability_analysis.py --experiment_dir PATH_TO_EXPERIMENT [--attack online]
"""

import argparse
import numpy as np
from pathlib import Path

# Import from local modules
from analysis_utils import (
    load_attack_scores,
    load_membership_labels,
    load_dataset_for_analysis,
    load_experiment_config,
    compute_per_sample_confusion_matrix,
    rank_samples_by_vulnerability,
    get_highly_vulnerable_samples
)
from visualization import plot_vulnerable_samples_grid


def analyze_vulnerability(experiment_dir: str,
                         attack_name: str = "LiRA (online)",
                         threshold: float = 0.0,
                         output_dir: str = None,
                         k_samples: int = 20) -> dict:
    """
    Analyze per-sample vulnerability to membership inference.

    Args:
        experiment_dir: Path to experiment directory
        attack_name: Attack variant to analyze
        threshold: Decision threshold
        output_dir: Output directory (defaults to experiment_dir)
        k_samples: Number of top vulnerable samples to visualize

    Returns:
        Dictionary with paths to generated files
    """
    experiment_dir = Path(experiment_dir)
    if output_dir is None:
        output_dir = experiment_dir / "vulnerability_analysis"
    else:
        output_dir = Path(output_dir)

    output_dir.mkdir(parents=True, exist_ok=True)

    # Load data
    print("Loading attack scores and labels...")
    scores_dict = load_attack_scores(experiment_dir, mode='leave_one_out')
    labels = load_membership_labels(experiment_dir)

    if attack_name not in scores_dict:
        available = list(scores_dict.keys())
        raise ValueError(
            f"Attack '{attack_name}' not found. Available: {available}"
        )

    scores = scores_dict[attack_name]
    print(f"Loaded scores: shape {scores.shape}")

    # Compute confusion matrix per sample
    print("Computing per-sample confusion matrices...")
    confusion_df = compute_per_sample_confusion_matrix(scores, labels, threshold)

    # Rank by vulnerability
    print("Ranking samples by vulnerability...")
    ranked_df = rank_samples_by_vulnerability(confusion_df, sort_by='low_fp_high_tp')

    # Get highly vulnerable samples
    highly_vulnerable = get_highly_vulnerable_samples(
        confusion_df, min_tp=1, max_fp=0
    )

    print(f"Total samples: {len(confusion_df)}")
    print(f"Highly vulnerable (TP>0, FP=0): {len(highly_vulnerable)}")

    output_files = {}

    # Save CSV files
    ranked_path = output_dir / f"vulnerability_ranked_{attack_name.replace(' ', '_')}.csv"
    ranked_df.to_csv(ranked_path, index=False)
    print(f"Saved ranked samples to {ranked_path}")
    output_files['ranked_csv'] = str(ranked_path)

    highly_vuln_path = output_dir / f"highly_vulnerable_{attack_name.replace(' ', '_')}.csv"
    highly_vulnerable.to_csv(highly_vuln_path, index=False)
    print(f"Saved highly vulnerable samples to {highly_vuln_path}")
    output_files['highly_vulnerable_csv'] = str(highly_vuln_path)

    # Load dataset for visualization (if image dataset)
    try:
        configs = load_experiment_config(experiment_dir)
        if 'train_config' in configs or 'attack_config' in configs:
            config = configs.get('train_config') or configs.get('attack_config')
            dataset_name = config.get('dataset', {}).get('name', '').lower()

            if dataset_name in ['cifar10', 'cifar100', 'gtsrb']:
                print("Loading dataset for visualization...")
                dataset, _ = load_dataset_for_analysis(config)

                if dataset is not None:
                    # Create vulnerability grid
                    print(f"Creating grid of top {k_samples} vulnerable samples...")

                    # Prepare confusion stats array
                    confusion_stats = confusion_df[['tp', 'fp']].values

                    grid_path = output_dir / f"top{k_samples}_vulnerable_grid.png"
                    plot_vulnerable_samples_grid(
                        sample_indices=ranked_df['sample_id'].values,
                        dataset=dataset,
                        confusion_stats=confusion_stats,
                        save_path=grid_path,
                        k=k_samples,
                        nrow=5,
                        title=f"Top {k_samples} Vulnerable Samples - {attack_name}"
                    )
                    output_files['grid_visualization'] = str(grid_path)

    except Exception as e:
        print(f"Warning: Could not create visualization: {e}")

    # Print summary statistics
    print("\n=== Vulnerability Summary ===")
    print(f"Top 10 most vulnerable samples:")
    print(ranked_df.head(10)[['sample_id', 'tp', 'fp', 'tn', 'fn']])

    print(f"\nDistribution of TP counts:")
    print(confusion_df['tp'].describe())

    print(f"\nDistribution of FP counts:")
    print(confusion_df['fp'].describe())

    return output_files


def main():
    """Command-line interface for vulnerability analysis."""
    parser = argparse.ArgumentParser(
        description="Analyze per-sample vulnerability to membership inference"
    )
    parser.add_argument(
        "--experiment_dir",
        type=str,
        required=True,
        help="Path to experiment directory"
    )
    parser.add_argument(
        "--attack",
        type=str,
        default="LiRA (online)",
        help="Attack variant to analyze (default: 'LiRA (online)')"
    )
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.0,
        help="Decision threshold (default: 0.0)"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default=None,
        help="Output directory (default: experiment_dir/vulnerability_analysis)"
    )
    parser.add_argument(
        "--k_samples",
        type=int,
        default=20,
        help="Number of top samples to visualize (default: 20)"
    )

    args = parser.parse_args()

    analyze_vulnerability(
        experiment_dir=args.experiment_dir,
        attack_name=args.attack,
        threshold=args.threshold,
        output_dir=args.output_dir,
        k_samples=args.k_samples
    )


if __name__ == "__main__":
    main()
