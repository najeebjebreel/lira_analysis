# This file contains unified settings for image datasets used in LiRA attacks
# Optimized for fine-tuning pretrained ResNet18 on CIFAR-10

# Random seed for reproducibility
seed: 3
use_cuda: true  # Whether to use CUDA for training

# Dataset configuration
dataset:
  name: cifar10  # Options: cifar10, cifar100, gtsrb, cinic10, imagenet
  num_classes: 10  # CIFAR-10 has 10 classes (was 43)
  input_size: 144  # Blanced accuracy/efficiency input size for pretrained models 
  data_dir: data  # Directory to store datasets
  pkeep: 0.5  # Probability of keeping a sample as a member

train_data_augmentation:
  - random_flip            # Adds input diversity and breaks spatial patterns
  - random_crop            # Reduces reliance on spatial memorization
  - random_rotation      # Optional: May be too aggressive for fine-tuning
  - color_jitter         # Optional: May be too aggressive for fine-tuning
  - normalize              # Essential - uses ImageNet stats when pretrained=true
  # - cutout               # Encourages robustness to occluded features
  # - mixup                # Blends samples, reducing overconfident predictions
  - cutmix               # Usually too aggressive for fine-tuning pretrained models

# Model configuration
model:
  architecture: resnet18  # Options: resnet18, resnet34, resnet50, wrn28-2, wrn28-10, efficientnetv2_rw_s, fcn, etc.
  input_channels: 3  # Number of input channels (RGB images)
  cifar_stem: false  # Set to false for pretrained models (use ImageNet stem)
  drop_rate: 0.1  
  pretrained: true  # Use ImageNet pretrained weights

# Training configuration
training:
  num_shadow_models: 256  
  epochs: 10  # 
  batch_size: 128
  num_workers: 6
  
  # AdamW optimizer - excellent with OneCycle
  optimizer: adamw
  learning_rate: 0.0005  # Base LR - OneCycle will peak at 3x this (0.0003)
  weight_decay: 0.001  # Higher weight decay for better regularization
  betas: [0.9, 0.999]  # AdamW beta parameters
  eps: 1e-8  # AdamW epsilon
  
  lr_scheduler: onecycle
  
  use_amp: true
  
  # Checkpointing
  save_step: 10  # Save more frequently with fewer epochs
  save_models: true
  
  # Early stopping
  early_stopping: true
  patience: 5  # Reduced patience for faster convergence
  
  resume: false

# Experiment configuration
experiment:
  log_level: info  # Logging level: debug, info, warning, error
  checkpoint_dir: none  # Options: 'none' to start from scratch or path to checkpoint directory

