
# This file contains unified settings for image datasets used in LiRA attacks

# Random seed for reproducibility
seed: 3 #optional: Set a random seed for reproducibility (default: 42)
use_cuda: true  # Whether to use CUDA for training

# Dataset configuration
dataset:
  name: cifar10  # Options: cifar10, cifar100, gtsrb, cinic10, imagenet
  num_classes: 10  # Options: 10, 100, 43, 10, 1000
  input_size: 32   # Input size for the model: 32 for training from scratch
  data_dir: data  # Directory to store datasets
  pkeep: 0.5  # Probability of keeping a sample as a member


train_data_augmentation:
  - random_flip            # Adds input diversity and breaks spatial patterns
  - random_crop            # Reduces reliance on spatial memorization
  - random_rotation        # Randomly rotate the image within a small degree range
  - color_jitter           # Reduces sensitivity to color features
  - normalize              # Essential baseline transformation
  # - cutout                 # Encourages robustness to occluded features
  # - mixup                  # Blends samples, reducing overconfident predictions
  - cutmix                 # Encourages spatial and semantic robustness


# Model configuration
model:
  architecture: resnet18  # Options: resnet18, resnet34, resnet50, wrn28-2, wrn28-10, efficientnetv2_rw_s ,fcn, ..etc.
  input_channels: 3  # Number of input channels (e.g., 3 for RGB images)
  cifar_stem: true  # Whether to use CIFAR stem 
  drop_rate: 0.1 # Dropout rate for the model
  pretrained: false  # Whether to use pretrained weights

# Training configuration
training:
  num_shadow_models: 256 # Number of shadow models to train
  epochs: 100  # Number of training epochs
  batch_size: 256  # Batch size for training
  num_workers: 4  # Number of workers for data loading
  optimizer: sgd
  learning_rate: 0.1  # Initial learning rate
  weight_decay: 1e-3 # Weight decay for regularization
  momentum: 0.9
  lr_scheduler: cosine  # Options: cosine
  save_step: 101  # Save model every n epochs
  save_models: true
  early_stopping: true
  patience: 20  # Number of epochs to wait for improvement before early stopping
  resume: false  # Whether to resume training from a checkpoint


# Experiment configuration
experiment:
  log_level: info
  checkpoint_dir: none  #Options: none to start from scratch or directory to load checkpoints for resume training and/or attack (e.g., experiments/cifar10/wrn28-2/2025-05-12_0056)
