# Comprehensive Analysis of the Likelihood Ratio MI Attack (LiRA)

This directory contains Jupyter notebooks and scripts for analyzing LiRA attack results.

## Contents

### Jupyter Notebooks

#### `comprehensive_analysis.ipynb`
**Purpose**: Primary analysis pipeline for LiRA attack evaluation with two-threshold modes and per-sample vulnerability assessment

**What it does**:
- **Data loading:**
  - Loads membership labels and attack scores from experiment directory
  - Validates data dimensions and integrity
  - Supports multiple attack variants (online, offline, fixed variance, global threshold)

- **Two-threshold evaluation modes:**
  - **Target mode**: Each model uses its own threshold computed from its ROC curve (upper bound performance)
  - **Shadow mode**: Each model uses the median threshold from other shadow models (realistic transferability scenario)

- **Per-model metrics:**
  - Computes confusion matrices (TP/FP/TN/FN) at target and shadow thresholds
  - Calculates TPR/FPR at specified operating points (e.g., 0.001% and 0.1% FPR)
  - Evaluates AUC for each model

- **Precision at multiple priors:**
  - Computes precision assuming different membership priors (1%, 10%, 50%)
  - Accounts for base rate effects using Bayes' theorem
  - Shows how prior probability affects attack interpretation

- **Per-sample vulnerability analysis:**
  - Computes TP/FP/TN/FN for each sample across all leave-one-out models
  - Ranks samples by vulnerability: low FP (stable) + high TP (reliably detected when member)
  - Identifies highly vulnerable samples (FP=0, TP>0): never false alarm, detected when present

- **Visualization:**
  - Generates grid of most vulnerable samples with TP/FP annotations
  - Customizable sample visualization for image datasets (CIFAR-10, CIFAR-100, GTSRB)

**Outputs generated** (saved to `analysis_results/{dataset}/{model}/{config}/`):
- `per_model_metrics_two_modes.csv`: Detailed per-model results for all attacks, FPRs, priors, and modes
- `summary_statistics_two_modes.csv`: Aggregated statistics with mean ± std across models
- `samples_vulnerability_ranked_online_shadow_0p001pct.csv`: All samples ranked by vulnerability (sorted by FP ascending, TP descending)
- `samples_highly_vulnerable_online_shadow_0p001pct.csv`: Subset of highly vulnerable samples (FP=0, TP>0)
- `top20_vulnerable_online_shadow_0p001pct.png`: Grid visualization of top 20 vulnerable samples with TP/FP badges

**Prerequisites:**
- Experiment directory must contain:
  - `membership_labels.npy`: Ground truth membership labels [M models × N samples]
  - Attack score files (e.g., `online_scores_leave_one_out.npy`)
  - `attack_config.yaml`: Configuration for dataset loading (for visualization only)

**Configuration:**
- Edit `Config` class in notebook to set:
  - `EXP_PATH`: Path to experiment directory
  - `TARGET_FPRS`: Operating points to evaluate
  - `PRIORS`: Membership priors for precision computation
  - `SCORE_FILES`: Attack variants to analyze

---

#### `threshold_distribution.ipynb`
**Purpose**: Visualize threshold stability across target models

**What it does**: 
- Loads threshold information from `per_model_metrics_two_modes.csv` (generated by `comprehensive_analysis.ipynb`)
- Creates box plots showing:
  - Single-run threshold distributions per attack and FPR
  - Multi-run threshold distributions across different targets
  - Comparison of target vs shadow threshold distributions
- Saves publication-quality figures

**Prerequisites:**
- Must run `comprehensive_analysis.ipynb` first to generate input files in `analysis_results/` folder

**Inputs:**
- `per_model_metrics_two_modes.csv`: Contains per-model thresholds for both target and shadow modes

---

#### `reproducibility.ipynb`
**Purpose**: Analyze reproducibility, stability, and coverage of vulnerable sample detection across runs

**What it does**: 
- Rebuilds the reproducibility/stability/coverage analyses from the paper (Fig. 2)
- Computes set-theoretic metrics over multiple experiment runs:
  - **Jaccard index**: |intersection| / |union| (agreement ratio)
  - **Intersection size**: Samples detected in ALL k runs (high-confidence positives)
  - **Union size**: Samples detected in ANY of k runs (broad coverage)
- Analyzes how agreement varies with:
  - Number of runs combined (k = 2, 3, ..., M)
  - Training variations (identical seeds vs different architectures/hyperparameters)
- Operates on 0FP detection sets (optionally filtered by TP≥x support threshold)

**Prerequisites:**
- Must run `comprehensive_analysis.ipynb` on multiple experiment configurations first
- Requires `samples_vulnerability_ranked_online_shadow_0p001pct.csv` from each run

**Inputs:**
- Multiple `samples_vulnerability_ranked_online_shadow_0p001pct.csv` files from different experiment runs
- Loads sample IDs and TP counts to build detection sets

**Key Analysis:**
- Compares scenarios (e.g., Fig. 3 in paper):
  - Identical training (only seed varies)
  - +1 difference (architecture OR augmentation OR regularization)
  - +2 differences (two factors vary)
  - +3 differences (three factors vary)

---

#### `loss_ratio_tpr.ipynb`
**Purpose**: Analyze relationship between loss ratios and TPR (Fig. 5 in paper)

**What it does**: 
- Examines correlation between model loss statistics and attack success
- Creates scatter plots and trend lines showing:
  - Loss ratio (member loss / non-member loss) vs TPR
  - How predictive loss ratios are for vulnerability
- Statistical analysis of loss-based vulnerability indicators

**Prerequisites:**
- Collect loss ratios and TPR values into a single CSV file before running
- CSV should contain columns: model_id, loss_ratio, tpr, attack_type, etc.

**Inputs:**
- Custom CSV with aggregated loss and TPR statistics across models/attacks

---

#### `plot_benchmark_distribution.ipynb`
**Purpose**: Compare score and likelihood distributions for individual samples across benchmarks (Fig. 6 in paper)

**What it does**:
- Visualizes how a specific sample's scores change across different training configurations
- Compares distributions for:
  - Baseline (standard training)
  - AOF (Adaptive Overfitting Fortification)
  - AOF+TL (AOF with Transfer Learning)
- Shows likelihood ratio distributions and detection consistency
- Helps understand defense effectiveness at the sample level

**Prerequisites:**
- Attack scores from multiple benchmark experiments
- Must specify sample ID to analyze

**Inputs:**
- Score files from multiple benchmarks (baseline, AOF, AOF+TL)
- Sample ID or index to visualize

---

## Typical Workflow

### 1. Primary Analysis (Required First)
```bash
# Run on your experiment
jupyter notebook comprehensive_analysis.ipynb
```
This generates all necessary outputs in `analysis_results/` folder.

### 2. Threshold Analysis (Optional)
```bash
jupyter notebook threshold_distribution.ipynb
```
Visualize threshold stability using outputs from step 1.

### 3. Reproducibility Analysis (Optional, requires multiple runs)
```bash
# First, run comprehensive_analysis.ipynb on multiple experiment configurations
# Then analyze cross-run agreement:
jupyter notebook reproducibility.ipynb
```

### 4. Loss Analysis (Optional, requires custom data collection)
```bash
# Collect loss statistics and TPR into CSV first
jupyter notebook loss_ratio_tpr.ipynb
```

### 5. Sample-Level Comparison (Optional)
```bash
jupyter notebook plot_benchmark_distribution.ipynb
```

---

## Output Directory Structure

```
analysis_results/
└── {dataset}/              # e.g., cifar10, cifar100
    └── {model}/            # e.g., resnet18, wrn28-2
        └── {config}/       # e.g., weak_aug, strong_aug
            ├── per_model_metrics_two_modes.csv
            ├── summary_statistics_two_modes.csv
            ├── samples_vulnerability_ranked_online_shadow_0p001pct.csv
            ├── samples_highly_vulnerable_online_shadow_0p001pct.csv
            └── top20_vulnerable_online_shadow_0p001pct.png
```

---

## Key Concepts

### Target vs Shadow Thresholds
- **Target**: Optimal threshold from model's own ROC (upper bound, assumes perfect knowledge)
- **Shadow**: Median threshold from other models (realistic, assumes transferability)
- Shadow mode typically shows 10-30% lower TPR but is more realistic for real attacks

### Vulnerability Ranking
Samples ranked by: `(FP ↑, TP ↓)` ascending FP, descending TP
- **Most vulnerable**: FP=0 (never false alarm), high TP (detected when member)
- **Least vulnerable**: High FP (often false alarm), low TP (rarely detected)

### Membership Priors
Precision depends heavily on base rate:
- Prior=0.5 (balanced): Precision ≈ TPR/(TPR+FPR)
- Prior=0.01 (1% members): Precision drastically reduced even with high TPR
- Always report precision at multiple priors for realistic scenarios

---

## Dependencies

```python
numpy
pandas
matplotlib
seaborn
scikit-learn
torch
torchvision
pyyaml
```

Install with:
```bash
pip install numpy pandas matplotlib seaborn scikit-learn torch torchvision pyyaml
```