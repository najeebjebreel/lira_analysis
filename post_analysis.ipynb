{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a10491d-178c-4aec-9ebf-dd4fa916262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import os\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torchvision.datasets import CIFAR10, CIFAR100, ImageFolder\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "from math import sqrt\n",
    "from scipy.stats import beta\n",
    "import re\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1214be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(config):\n",
    "    dataset_name = config['dataset']['name'].lower()\n",
    "    data_dir = \"D:/mona/mia_research/data\"\n",
    "    \n",
    "    if dataset_name == 'cifar10':\n",
    "        # Directly apply transform during loading for inference\n",
    "        train_dataset = CIFAR10(root=data_dir, train=True, download=False)\n",
    "        test_dataset = CIFAR10(root=data_dir, train=False, download=False)\n",
    "        full_dataset = ConcatDataset([train_dataset, test_dataset])\n",
    "        train_label = np.array(train_dataset.targets)\n",
    "        test_label = np.array(test_dataset.targets)\n",
    "        full_label = np.concatenate((train_label, test_label), axis=0)\n",
    "    \n",
    "    elif dataset_name == 'cifar100':\n",
    "        train_dataset = CIFAR100(root=data_dir, train=True, download=False)\n",
    "        test_dataset = CIFAR100(root=data_dir, train=False, download=False)\n",
    "        full_dataset = ConcatDataset([train_dataset, test_dataset])\n",
    "        train_label = np.array(train_dataset.targets)\n",
    "        test_label = np.array(test_dataset.targets)\n",
    "        full_label = np.concatenate((train_label, test_label), axis=0)\n",
    "           \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
    "\n",
    "    return full_dataset, full_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a182e3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " - analysis_results\\cifar10\\wrn28-2\\weak_rotate_jitter_cutmix_drop0.1_wd1e-3\\per_model_metrics_two_modes.csv\n",
      " - analysis_results\\cifar10\\wrn28-2\\weak_rotate_jitter_cutmix_drop0.1_wd1e-3\\summary_statistics_two_modes.csv\n"
     ]
    }
   ],
   "source": [
    "# TWO-MODE EVAL: target-based & shadow-based\n",
    "# - Recompute τ directly from scores (no CSV rounding)\n",
    "# - Use scikit-learn ROC semantics: score >= τ\n",
    "# - \"Global threshold\" attack: only target-mode baseline is produced\n",
    "\n",
    "# -------- CONFIG --------\n",
    "exp_path = Path(\"experiments/cifar10/wrn28-2/weak_rotate_jitter_cutmix_drop0.1_wd1e-3\")\n",
    "TARGET_FPRS = [0.00001, 0.001]   # 0.001% and 0.1%\n",
    "PRIORS = [0.01, 0.1, 0.5]\n",
    "DO_SANITY_CHECKS = True          # set False to skip exact ROC point checks\n",
    "\n",
    "SCORE_FILES = {\n",
    "    \"LiRA (online)\":               \"online_scores_leave_one_out.npy\",\n",
    "    \"LiRA (online, fixed var)\":    \"online_fixed_scores_leave_one_out.npy\",\n",
    "    \"LiRA (offline)\":              \"offline_scores_leave_one_out.npy\",\n",
    "    \"LiRA (offline, fixed var)\":   \"offline_fixed_scores_leave_one_out.npy\",\n",
    "    \"Global threshold\":            \"global_scores_leave_one_out.npy\",\n",
    "}\n",
    "LABELS_NPY = \"membership_labels.npy\"\n",
    "\n",
    "# -------- OUTPUT DIR --------\n",
    "def make_out_dir(src: Path) -> Path:\n",
    "    parts = src.parts\n",
    "    if len(parts) >= 4:\n",
    "        dataset, model, configs = parts[-3], parts[-2], parts[-1]\n",
    "    else:\n",
    "        dataset, model, configs = \"unknown_dataset\", \"unknown_model\", src.name\n",
    "    out = Path(\"analysis_results\") / dataset / model / configs\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "    return out\n",
    "\n",
    "out_dir = make_out_dir(exp_path)\n",
    "\n",
    "# -------- LOAD --------\n",
    "labels = np.load(exp_path / LABELS_NPY)  # [M, N], bool\n",
    "labels = labels.astype(bool, copy=False)\n",
    "scores = {name: np.load(exp_path / fname) for name, fname in SCORE_FILES.items()}\n",
    "M, N = labels.shape\n",
    "for name, arr in scores.items():\n",
    "    if arr.shape != (M, N):\n",
    "        raise ValueError(f\"{name} shape {arr.shape} != {(M, N)}\")\n",
    "\n",
    "# -------- HELPERS --------\n",
    "def compute_metrics_at_tau(scores_row: np.ndarray,\n",
    "                          labels_row: np.ndarray,\n",
    "                          tau: float):\n",
    "    \"\"\"\n",
    "    Compute confusion matrix and rates at threshold tau.\n",
    "    Returns tp, fp, tn, fn, tpr, fpr_achieved (NOT precision - computed later with prior).\n",
    "    \"\"\"\n",
    "    pred = scores_row >= tau\n",
    "    tp = int(np.sum(pred & labels_row))\n",
    "    fp = int(np.sum(pred & ~labels_row))\n",
    "    tn = int(np.sum(~pred & ~labels_row))\n",
    "    fn = int(np.sum(~pred & labels_row))\n",
    "    \n",
    "    tpr = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    fpr_achieved = fp / (fp + tn) if (fp + tn) else 0.0\n",
    "    \n",
    "    return tp, fp, tn, fn, tpr, fpr_achieved\n",
    "\n",
    "def compute_precision(tpr: float, fpr_achieved: float, prior: float):\n",
    "    \"\"\"\n",
    "    Compute precision given TPR, achieved FPR, and membership prior.\n",
    "    Assumes TPR and FPR are representative from balanced evaluation.\n",
    "    \"\"\"\n",
    "    ppos = tpr * prior + fpr_achieved * (1 - prior)\n",
    "    if ppos > 0:\n",
    "        return (tpr * prior) / ppos\n",
    "    else:\n",
    "        return np.nan  # Undefined when no positive predictions\n",
    "\n",
    "def tau_for_target(scores_row: np.ndarray,\n",
    "                   labels_row: np.ndarray,\n",
    "                   tfpr: float):\n",
    "    \"\"\"\n",
    "    Return (tau, fpr_at_tau, tpr_at_tau) where tau is the largest threshold such that\n",
    "    FPR(score >= tau) <= tfpr. Uses all thresholds (drop_intermediate=False).\n",
    "    \"\"\"\n",
    "    fpr, tpr, thr = roc_curve(labels_row.astype(bool),\n",
    "                              scores_row,\n",
    "                              drop_intermediate=False)\n",
    "    idx = np.where(fpr <= tfpr)[0]\n",
    "    if idx.size == 0:\n",
    "        return np.inf, None, None\n",
    "    j = idx[-1]\n",
    "    return float(thr[j]), float(fpr[j]), float(tpr[j])\n",
    "\n",
    "def median_others(arr: np.ndarray, exclude_idx: int) -> float:\n",
    "    \"\"\"Median of finite values in arr excluding arr[exclude_idx]. Returns inf if empty.\"\"\"\n",
    "    pool = np.delete(arr, exclude_idx)\n",
    "    pool = pool[np.isfinite(pool)]\n",
    "    return float(np.median(pool)) if pool.size else np.inf\n",
    "\n",
    "# -------- EVAL (two modes) --------\n",
    "rows_detail = []\n",
    "\n",
    "for attack_name, arr in scores.items():\n",
    "    # Precompute per-model AUC once (independent of τ / priors / mode)\n",
    "    aucs = np.full(M, np.nan, dtype=float)\n",
    "    for m in range(M):\n",
    "        try:\n",
    "            aucs[m] = roc_auc_score(labels[m].astype(int), arr[m])\n",
    "        except ValueError:\n",
    "            # happens if a model has only one class present; leave NaN\n",
    "            pass\n",
    "\n",
    "    for tfpr in TARGET_FPRS:\n",
    "        # 1) Target τ per model from its own ROC\n",
    "        taus_target = np.empty(M, dtype=np.float64)\n",
    "        fprs_chk = np.empty(M, dtype=np.float64)\n",
    "        tprs_chk = np.empty(M, dtype=np.float64)\n",
    "        fprs_chk[:] = np.nan\n",
    "        tprs_chk[:] = np.nan\n",
    "\n",
    "        for m in range(M):\n",
    "            tau_m, fpr_m, tpr_m = tau_for_target(arr[m], labels[m], tfpr)\n",
    "            taus_target[m] = tau_m\n",
    "            if fpr_m is not None:\n",
    "                fprs_chk[m] = fpr_m\n",
    "                tprs_chk[m] = tpr_m\n",
    "\n",
    "        # 2) Shadow τ per target = median of other models' target τs\n",
    "        taus_shadow = np.array([median_others(taus_target, m) for m in range(M)], dtype=np.float64)\n",
    "\n",
    "        # 3) Sanity checks (optional)\n",
    "        if DO_SANITY_CHECKS:\n",
    "            finite_idxs = np.where(np.isfinite(taus_target))[0]\n",
    "            for m in finite_idxs[:5]:\n",
    "                tau_m = taus_target[m]\n",
    "                _, _, _, _, tpr_re, fpr_re = compute_metrics_at_tau(arr[m], labels[m], tau_m)\n",
    "                # allow tiny numerical noise\n",
    "                if not (np.isclose(fpr_re, fprs_chk[m], atol=1e-12) and\n",
    "                        np.isclose(tpr_re, tprs_chk[m], atol=1e-12)):\n",
    "                    print(f\"[SanityCheck WARN] {attack_name} model {m} @ {tfpr}: \"\n",
    "                          f\"recomp (fpr={fpr_re}, tpr={tpr_re}) vs roc (fpr={fprs_chk[m]}, tpr={tprs_chk[m]})\")\n",
    "\n",
    "        # 4) Compute metrics for both modes\n",
    "        # TARGET mode\n",
    "        for m in range(M):\n",
    "            tau_m = taus_target[m]\n",
    "            if not np.isfinite(tau_m):\n",
    "                tp=fp=0; tn=int(np.sum(~labels[m])); fn=int(np.sum(labels[m]))\n",
    "                tpr=fpr_achieved=0.0\n",
    "            else:\n",
    "                tp, fp, tn, fn, tpr, fpr_achieved = compute_metrics_at_tau(arr[m], labels[m], tau_m)\n",
    "            \n",
    "            # Compute precision for each prior\n",
    "            for prior in PRIORS:\n",
    "                prec = compute_precision(tpr, fpr_achieved, prior)\n",
    "                rows_detail.append(dict(\n",
    "                    mode=\"target\", \n",
    "                    attack=attack_name, \n",
    "                    target_fpr=tfpr,\n",
    "                    achieved_fpr=fpr_achieved,  # FPR' - explicitly labeled\n",
    "                    prior=prior,\n",
    "                    model_idx=m, \n",
    "                    threshold=tau_m, \n",
    "                    tp=tp, fp=fp, tn=tn, fn=fn,\n",
    "                    tpr=tpr,\n",
    "                    precision=prec,\n",
    "                    auc=aucs[m]\n",
    "                ))\n",
    "\n",
    "        # SHADOW mode (skip for the baseline \"Global threshold\" attack)\n",
    "        if attack_name != \"Global threshold\":\n",
    "            for m in range(M):\n",
    "                tau_m = taus_shadow[m]\n",
    "                if not np.isfinite(tau_m):\n",
    "                    tp=fp=0; tn=int(np.sum(~labels[m])); fn=int(np.sum(labels[m]))\n",
    "                    tpr=fpr_achieved=0.0\n",
    "                else:\n",
    "                    tp, fp, tn, fn, tpr, fpr_achieved = compute_metrics_at_tau(arr[m], labels[m], tau_m)\n",
    "                \n",
    "                # Compute precision for each prior\n",
    "                for prior in PRIORS:\n",
    "                    prec = compute_precision(tpr, fpr_achieved, prior)\n",
    "                    rows_detail.append(dict(\n",
    "                        mode=\"shadow\",\n",
    "                        attack=attack_name,\n",
    "                        target_fpr=tfpr,\n",
    "                        achieved_fpr=fpr_achieved,  # FPR' - explicitly labeled\n",
    "                        prior=prior,\n",
    "                        model_idx=m,\n",
    "                        threshold=tau_m,\n",
    "                        tp=tp, fp=fp, tn=tn, fn=fn,\n",
    "                        tpr=tpr,\n",
    "                        precision=prec,\n",
    "                        auc=aucs[m]\n",
    "                    ))\n",
    "\n",
    "# -------- SAVE DETAIL + SUMMARY --------\n",
    "detail_df = pd.DataFrame(rows_detail)\n",
    "detail_path = out_dir / \"per_model_metrics_two_modes.csv\"\n",
    "detail_df.to_csv(detail_path, index=False)\n",
    "\n",
    "# Summary aggregation\n",
    "summary = (detail_df\n",
    "    .groupby([\"mode\",\"attack\",\"target_fpr\",\"prior\"], as_index=False)\n",
    "    .agg(TPR_Mean=(\"tpr\",\"mean\"), \n",
    "         TPR_Std=(\"tpr\",\"std\"),\n",
    "         FPR_Achieved_Mean=(\"achieved_fpr\",\"mean\"),  # Changed: explicitly \"achieved\"\n",
    "         FPR_Achieved_Std=(\"achieved_fpr\",\"std\"),\n",
    "         Precision_Mean=(\"precision\",\"mean\"),  # Uses nanmean implicitly\n",
    "         Precision_Std=(\"precision\",\"std\"),    # Uses nanstd implicitly\n",
    "         AUC_Mean=(\"auc\",\"mean\"), \n",
    "         AUC_Std=(\"auc\",\"std\"))\n",
    ")\n",
    "\n",
    "# Express rates & AUC as %\n",
    "for c in [\"TPR_Mean\",\"TPR_Std\",\"FPR_Achieved_Mean\",\"FPR_Achieved_Std\",\n",
    "          \"Precision_Mean\",\"Precision_Std\",\"AUC_Mean\",\"AUC_Std\"]:\n",
    "    summary[c] = (summary[c]*100).round(3)\n",
    "\n",
    "summary[\"Target FPR (%)\"] = (summary[\"target_fpr\"]*100).round(4)\n",
    "summary = summary.drop(columns=[\"target_fpr\"])\n",
    "\n",
    "# Reorder columns for clarity matching paper tables\n",
    "summary = summary[[\"mode\",\"attack\",\"Target FPR (%)\",\"prior\",\n",
    "                   \"TPR_Mean\",\"TPR_Std\",\n",
    "                   \"FPR_Achieved_Mean\",\"FPR_Achieved_Std\",\n",
    "                   \"Precision_Mean\",\"Precision_Std\",\n",
    "                   \"AUC_Mean\",\"AUC_Std\"]]\n",
    "\n",
    "summary_path = out_dir / \"summary_statistics_two_modes.csv\"\n",
    "summary.to_csv(summary_path, index=False)\n",
    "\n",
    "print(f\"Saved:\\n - {detail_path}\\n - {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ff250d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " - analysis_results\\cifar10\\wrn28-2\\weak_rotate_jitter_cutmix_drop0.1_wd1e-3\\samples_vulnerability_ranked_online_shadow_0p001pct.csv (60000 samples)\n",
      " - analysis_results\\cifar10\\wrn28-2\\weak_rotate_jitter_cutmix_drop0.1_wd1e-3\\samples_highly_vulnerable_online_shadow_0p001pct.csv (875 highly vulnerable samples)\n",
      "\n",
      "Most vulnerable sample: TP=0, FP=0\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# PER-SAMPLE VULNERABILITY (ONLINE @ 0.001% FPR, using SHADOW τ)\n",
    "# Computes TP, FP, TN, FN for each sample across leave-one-out models\n",
    "# -------------------------\n",
    "\n",
    "ATTACK_ONLINE = \"LiRA (online)\"\n",
    "FPR_001pct = 1e-5\n",
    "\n",
    "# Safety checks\n",
    "assert 'detail_df' in globals(), \"detail_df not found; run the two-mode cell first.\"\n",
    "assert 'scores' in globals() and ATTACK_ONLINE in scores, \"scores for LiRA (online) not found.\"\n",
    "assert 'labels' in globals(), \"labels not found.\"\n",
    "\n",
    "labels_bool = labels.astype(bool, copy=False)\n",
    "\n",
    "# 1) Get shadow thresholds per model\n",
    "mask = (\n",
    "    (detail_df[\"mode\"] == \"shadow\") &\n",
    "    (detail_df[\"attack\"] == ATTACK_ONLINE) &\n",
    "    np.isclose(detail_df[\"target_fpr\"].to_numpy(), FPR_001pct, atol=1e-12)\n",
    ")\n",
    "shadow_rows = detail_df.loc[mask, [\"model_idx\", \"threshold\"]].drop_duplicates(subset=[\"model_idx\"])\n",
    "\n",
    "if shadow_rows.empty:\n",
    "    raise RuntimeError(\"No shadow thresholds in detail_df for LiRA (online) @ 0.001% FPR.\")\n",
    "\n",
    "M_detected, N_detected = scores[ATTACK_ONLINE].shape\n",
    "taus_shadow = np.full(M_detected, np.inf, dtype=float)\n",
    "for _, r in shadow_rows.iterrows():\n",
    "    m = int(r[\"model_idx\"])\n",
    "    if 0 <= m < M_detected:\n",
    "        taus_shadow[m] = float(r[\"threshold\"])\n",
    "\n",
    "# 2) Predict per model with shadow thresholds\n",
    "scores_online = scores[ATTACK_ONLINE]\n",
    "pred = (scores_online >= taus_shadow[:, None])  # [M, N] boolean predictions\n",
    "\n",
    "# 3) Compute confusion matrix per sample\n",
    "TP = np.sum(pred & labels_bool, axis=0).astype(int)\n",
    "FP = np.sum(pred & ~labels_bool, axis=0).astype(int)\n",
    "TN = np.sum(~pred & ~labels_bool, axis=0).astype(int)\n",
    "FN = np.sum(~pred & labels_bool, axis=0).astype(int)\n",
    "\n",
    "# 4) Create dataframe with confusion matrix statistics\n",
    "sample_df = pd.DataFrame({\n",
    "    \"sample_id\": np.arange(N_detected),\n",
    "    \"tp\": TP,\n",
    "    \"fp\": FP,\n",
    "    \"tn\": TN,\n",
    "    \"fn\": FN,\n",
    "})\n",
    "\n",
    "# 5) Sort by vulnerability: prioritize low FP (stable non-false-positives), then high TP (high detection)\n",
    "# This gives samples that are reliably detected when members, rarely flagged when non-members\n",
    "vulnerability_sorted = sample_df.sort_values(by=[\"fp\", \"tp\"], ascending=[True, False])\n",
    "vuln_path = out_dir / \"samples_vulnerability_ranked_online_shadow_0p001pct.csv\"\n",
    "vulnerability_sorted.to_csv(vuln_path, index=False)\n",
    "\n",
    "# 6) Most vulnerable: FP=0 (never false alarm) AND TP>0 (detected when member)\n",
    "highly_vulnerable = sample_df[(sample_df[\"fp\"] == 0) & (sample_df[\"tp\"] > 0)]\n",
    "high_vuln_path = out_dir / \"samples_highly_vulnerable_online_shadow_0p001pct.csv\"\n",
    "highly_vulnerable.to_csv(high_vuln_path, index=False)\n",
    "\n",
    "print(f\"Saved:\\n - {vuln_path} ({len(sample_df)} samples)\")\n",
    "print(f\" - {high_vuln_path} ({len(highly_vulnerable)} highly vulnerable samples)\")\n",
    "print(f\"\\nMost vulnerable sample: TP={sample_df.iloc[0]['tp']}, FP={sample_df.iloc[0]['fp']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "007569b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _to_chw_float_tensor(img):\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        t = img.clone()\n",
    "        if t.ndim == 2: t = t.unsqueeze(0)\n",
    "        elif t.ndim == 3 and t.shape[0] not in (1,3): t = t.permute(2,0,1)\n",
    "        t = t.float()\n",
    "        if t.numel() and t.max() > 1.0: t = t / 255.0\n",
    "    else:\n",
    "        arr = np.array(img)\n",
    "        t = torch.from_numpy(arr)\n",
    "        if t.ndim == 2: t = t.unsqueeze(-1)\n",
    "        if t.ndim == 3 and t.shape[-1] in (1,3): t = t.permute(2,0,1)\n",
    "        t = t.float()\n",
    "        if t.numel() and t.max() > 1.0: t = t / 255.0\n",
    "    if t.shape[0] == 1: t = t.repeat(3,1,1)\n",
    "    elif t.shape[0] > 3: t = t[:3]\n",
    "    return t\n",
    "\n",
    "def display_top_k_vulnerable_samples(\n",
    "    vulnerable_samples, full_dataset,\n",
    "    k: int = 20, nrow: int = 5,\n",
    "    padding: int = 2, normalize: bool = True, dpi: int = 300,\n",
    "    out_dir: \"Path|str\" = \".\", save_name: str = \"vulnerable_samples.png\",\n",
    "    sample_id_col: str = \"sample_id\",\n",
    "    font_size: int = 8,\n",
    "    badge_margin: int = 2,      # distance from the tile corner\n",
    "    overhang_left: int = 6,     # extra pixels LEFT (can place into padding/outside)\n",
    "    overhang_up: int = 6,       # extra pixels UP\n",
    "):\n",
    "    \"\"\"\n",
    "    Grid of top-k samples with a 'TP:.. FP:..' badge in the TOP-LEFT.\n",
    "    'overhang_left'/'overhang_up' move the badge further into the corner.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    if not isinstance(vulnerable_samples, pd.DataFrame):\n",
    "        raise TypeError(\"vulnerable_samples must be a pandas DataFrame\")\n",
    "    for col in (sample_id_col, \"tp\", \"fp\"):\n",
    "        if col not in vulnerable_samples.columns:\n",
    "            raise KeyError(f\"Column '{col}' missing from vulnerable_samples\")\n",
    "\n",
    "    vs = vulnerable_samples.head(k).copy()\n",
    "    ids = vs[sample_id_col].to_numpy()\n",
    "\n",
    "    images = [_to_chw_float_tensor(full_dataset[int(sid)][0]) for sid in ids]\n",
    "    tensor = torch.stack(images)  # [k, 3, H, W]\n",
    "\n",
    "    grid = vutils.make_grid(tensor, nrow=nrow, padding=padding, normalize=normalize, pad_value=1.0)\n",
    "    grid_np = grid.permute(1,2,0).cpu().numpy()\n",
    "\n",
    "    rows = (k + nrow - 1) // nrow\n",
    "    H, W = tensor.shape[-2], tensor.shape[-1]\n",
    "    fig_w = max(4, nrow * 1.2)\n",
    "    fig_h = max(4, rows * 1.2)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(fig_w, fig_h), dpi=dpi)\n",
    "    ax.imshow(grid_np, aspect=\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # grid math per torchvision.make_grid\n",
    "    stride_x, stride_y = W + padding, H + padding\n",
    "    base_x = base_y = padding\n",
    "\n",
    "    # annotate top-left corner\n",
    "    for i in range(len(ids)):\n",
    "        r, c = divmod(i, nrow)\n",
    "        x0 = base_x + c * stride_x    # tile's left edge\n",
    "        y0 = base_y + r * stride_y    # tile's top edge\n",
    "\n",
    "        tp_val = int(vs.iloc[i][\"tp\"])\n",
    "        fp_val = int(vs.iloc[i][\"fp\"])\n",
    "        text = f\"TP:{tp_val}  FP:{fp_val}\"\n",
    "\n",
    "        # place slightly inside the tile, then overhang into the corner\n",
    "        x_text = x0 + badge_margin - overhang_left\n",
    "        y_text = y0 + badge_margin - overhang_up\n",
    "\n",
    "        ax.text(\n",
    "            x_text, y_text, text,\n",
    "            ha=\"left\", va=\"top\",\n",
    "            fontsize=font_size, fontweight=\"bold\",\n",
    "            bbox=dict(boxstyle=\"round,pad=0.25\", facecolor=\"white\", alpha=0.9,\n",
    "                      edgecolor=\"black\", linewidth=0.7),\n",
    "            color=\"black\",\n",
    "            clip_on=False,   # allow overhang beyond axes/tile\n",
    "        )\n",
    "\n",
    "    plt.tight_layout(pad=0.1)\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = out_dir / save_name\n",
    "    fig.savefig(out_path, bbox_inches=\"tight\", dpi=dpi, facecolor=\"white\")\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved grid: {out_path}\")\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49454b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples ranked: 60000\n",
      "Top 5 most vulnerable:\n",
      "       sample_id  tp  fp   tn  fn\n",
      "38479      38479  67   0  128  61\n",
      "40350      40350  62   0  128  66\n",
      "47417      47417  59   0  128  69\n",
      "47063      47063  58   0  128  70\n",
      "10361      10361  56   0  128  72\n",
      "Saved grid: analysis_results\\cifar10\\wrn28-2\\weak_rotate_jitter_cutmix_drop0.1_wd1e-3\\top20_vulnerable_online_shadow_0p001pct.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('analysis_results/cifar10/wrn28-2/weak_rotate_jitter_cutmix_drop0.1_wd1e-3/top20_vulnerable_online_shadow_0p001pct.png')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load config and dataset\n",
    "cfg_path = exp_path / \"attack_config.yaml\"\n",
    "if not cfg_path.exists():\n",
    "    raise FileNotFoundError(f\"Config not found at {cfg_path}\")\n",
    "\n",
    "with open(cfg_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "full_dataset, _ = load_dataset(config)\n",
    "\n",
    "# vulnerability_sorted should already be in memory from previous cell\n",
    "# This fallback is just for safety if running cells out of order\n",
    "if \"vulnerability_sorted\" not in globals():\n",
    "    rank_csv = out_dir / \"samples_vulnerability_ranked_online_shadow_0p001pct.csv\"\n",
    "    if not rank_csv.exists():\n",
    "        raise FileNotFoundError(f\"Ranking CSV not found at {rank_csv}. \"\n",
    "                                \"Run the per-sample vulnerability cell first.\")\n",
    "    vulnerability_sorted = pd.read_csv(rank_csv)\n",
    "\n",
    "print(f\"Total samples ranked: {len(vulnerability_sorted)}\")\n",
    "print(f\"Top 5 most vulnerable:\")\n",
    "print(vulnerability_sorted.head()[[\"sample_id\", \"tp\", \"fp\", \"tn\", \"fn\"]])\n",
    "\n",
    "# Visualize top 20 most vulnerable\n",
    "display_top_k_vulnerable_samples(\n",
    "    vulnerable_samples=vulnerability_sorted,\n",
    "    full_dataset=full_dataset,\n",
    "    k=20,\n",
    "    nrow=5,\n",
    "    out_dir=out_dir,\n",
    "    save_name=\"top20_vulnerable_online_shadow_0p001pct.png\",\n",
    "    font_size=7,\n",
    "    badge_margin=2,\n",
    "    overhang_left=3,\n",
    "    overhang_up=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1f82cccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table*}[!ht]\n",
      "\\centering\n",
      "\\caption{CIFAR-100 with balanced prior and target-based threshold. TPR reduction factors relative to baseline shown in parentheses for  (TL).}\n",
      "\\label{tab:gtsrb-reg}\n",
      "\\resizebox{\\textwidth}{!}{%\n",
      "\\begin{tabular}{lcccc}\n",
      "\\toprule\n",
      "Benchmark & Attack & TPR@ & TPR@ & AUC \\\\ & & 0.001\\% FPR (\\%) & 0.1\\% FPR (\\%) & (\\%) \\\\ \n",
      "\\midrule\n",
      "\\multirow{5}{*}{Purchase-100 (baseline)} & Online & 0.523 $\\pm$ 0.243 & 4.491 $\\pm$ 0.281 & 70.163 $\\pm$ 0.286 \\\\\n",
      "& Online (fixed var) & 0.180 $\\pm$ 0.110 & 3.089 $\\pm$ 0.188 & 69.521 $\\pm$ 0.280 \\\\\n",
      "& Offline & 0.007 $\\pm$ 0.007 & 0.500 $\\pm$ 0.077 & 55.105 $\\pm$ 0.484 \\\\\n",
      "& Offline (fixed var) & 0.022 $\\pm$ 0.017 & 0.713 $\\pm$ 0.078 & 56.110 $\\pm$ 0.506 \\\\\n",
      "& Global threshold & 0.001 $\\pm$ 0.001 & 0.100 $\\pm$ 0.015 & 54.834 $\\pm$ 0.148 \\\\\n",
      "\\midrule\n",
      "\\multirow{5}{*}{Purchase-100 (reg.)} & Online & 0.022 $\\pm$ 0.017 ($\\times$24) & 0.825 $\\pm$ 0.068 ($\\times$5.4) & 62.640 $\\pm$ 0.160 \\\\\n",
      "& Online (fixed var) & 0.026 $\\pm$ 0.019 ($\\times$6.9) & 0.794 $\\pm$ 0.067 ($\\times$3.9) & 62.816 $\\pm$ 0.162 \\\\\n",
      "& Offline & 0.001 $\\pm$ 0.001 ($\\times$7.0) & 0.139 $\\pm$ 0.022 ($\\times$3.6) & 51.879 $\\pm$ 0.175 \\\\\n",
      "& Offline (fixed var) & 0.003 $\\pm$ 0.003 ($\\times$7.3) & 0.230 $\\pm$ 0.031 ($\\times$3.1) & 52.502 $\\pm$ 0.188 \\\\\n",
      "& Global threshold & 0.001 $\\pm$ 0.002 ($\\times$1.0) & 0.101 $\\pm$ 0.014 ($\\times$1.0) & 52.199 $\\pm$ 0.128 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}%\n",
      "}\n",
      "\\end{table*}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Build an aggregated LaTeX table across benchmarks:\n",
    "# Balanced prior (0.5), target-based thresholds, show:\n",
    "#   - TPR @ 0.001% FPR\n",
    "#   - TPR @ 0.1% FPR\n",
    "#   - AUC\n",
    "# Add reduction factors (×...) vs baseline per attack & FPR\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# ===== USER CONFIG =====\n",
    "# Map display name -> CSV path (first is baseline)\n",
    "BENCHMARKS = {\n",
    "    \"Purchase-100 (baseline)\": \"analysis_results/purchase100/fcn/2025-05-29_1623_drp_0_wd_5e-5/summary_statistics_two_modes.csv\",\n",
    "    \"Purchase-100 (reg.)\":     \"analysis_results/purchase100/fcn/2025-06-15_2326_drp_0.5_wd_1e-3/summary_statistics_two_modes.csv\",\n",
    "    # \"CIFAR-100 (TL)\":       \"analysis_results/cifar100/efficientnetv2_rw_s/tl/summary_statistics_two_modes.csv\",\n",
    "}\n",
    "\n",
    "\n",
    "# Which attacks (and order) to show\n",
    "ATTACK_ORDER = [\n",
    "    \"LiRA (online)\",\n",
    "    \"LiRA (online, fixed var)\",\n",
    "    \"LiRA (offline)\",\n",
    "    \"LiRA (offline, fixed var)\",\n",
    "    \"Global threshold\",\n",
    "]\n",
    "\n",
    "# Pretty names for the attack column\n",
    "ATTACK_DISPLAY = {\n",
    "    \"LiRA (online)\": \"Online\",\n",
    "    \"LiRA (online, fixed var)\": \"Online (fixed var)\",\n",
    "    \"LiRA (offline)\": \"Offline\",\n",
    "    \"LiRA (offline, fixed var)\": \"Offline (fixed var)\",\n",
    "    \"Global threshold\": \"Global threshold\",\n",
    "}\n",
    "\n",
    "PRIOR = 0.5             # balanced prior\n",
    "MODE  = \"target\"        # target-based threshold only\n",
    "ALPHA1 = 0.001          # TPR @ 0.001% FPR (as percent)\n",
    "ALPHA2 = 0.1            # TPR @ 0.1% FPR (as percent)\n",
    "\n",
    "# Table meta\n",
    "CAPTION = (\"CIFAR-100 with balanced prior and target-based threshold. \"\n",
    "           \"TPR reduction factors relative to baseline shown in parentheses for  (TL).\")\n",
    "LABEL   = \"tab:gtsrb-reg\"\n",
    "COLUMN_FORMAT = \"lcccc\"  # Benchmark, Attack, TPR@0.001, TPR@0.1, AUC\n",
    "DIGITS_MAIN = 3          # mean ± std digits\n",
    "DIGITS_MUL  = 1          # ×multiplier digits (>=10 as integer)\n",
    "\n",
    "# ===== HELPERS =====\n",
    "def fmt_mu_sd(mu, sd, d=DIGITS_MAIN):\n",
    "    if pd.isna(mu) or pd.isna(sd):\n",
    "        return \"--\"\n",
    "    return f\"{mu:.{d}f} $\\\\pm$ {sd:.{d}f}\"\n",
    "\n",
    "def fmt_multiplier(r, d=DIGITS_MUL):\n",
    "    if r is None or np.isinf(r) or np.isnan(r):\n",
    "        return \"($\\\\times\\\\infty$)\"\n",
    "    if r >= 10:\n",
    "        return f\"($\\\\times${int(round(r))})\"\n",
    "    return f\"($\\\\times${r:.{d}f})\"\n",
    "\n",
    "def safe_ratio(baseline, variant):\n",
    "    # baseline / variant, guarding zeros\n",
    "    if variant is None or pd.isna(variant) or variant <= 0:\n",
    "        return np.inf\n",
    "    if baseline is None or pd.isna(baseline) or baseline < 0:\n",
    "        return np.nan\n",
    "    return baseline / variant\n",
    "\n",
    "def load_summary(csv_path: str, bench_name: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"Benchmark\"] = bench_name\n",
    "    return df\n",
    "\n",
    "def pick_single(df: pd.DataFrame, attack: str, alpha_percent: float):\n",
    "    \"\"\"Return (TPR_mu, TPR_sd) for given attack at the given Target FPR (%).\"\"\"\n",
    "    sub = df[(df[\"attack\"] == attack) &\n",
    "             (df[\"mode\"] == MODE) &\n",
    "             (np.isclose(df[\"prior\"], PRIOR, atol=1e-12)) &\n",
    "             (np.isclose(df[\"Target FPR (%)\"], alpha_percent, atol=1e-6))]\n",
    "    if sub.empty:\n",
    "        return (np.nan, np.nan)\n",
    "    # Already aggregated in CSV (mean±std across models). Take the mean of means (robust if repeated).\n",
    "    return (sub[\"TPR_Mean\"].mean(), sub[\"TPR_Std\"].mean())\n",
    "\n",
    "def pick_auc(df: pd.DataFrame, attack: str):\n",
    "    \"\"\"Return (AUC_mu, AUC_sd) for given attack (mode/alpha/prior independent).\"\"\"\n",
    "    sub = df[(df[\"attack\"] == attack) & (df[\"mode\"] == MODE)]\n",
    "    if sub.empty:\n",
    "        return (np.nan, np.nan)\n",
    "    # AUC shouldn't depend on prior/alpha; average if repeated\n",
    "    return (sub[\"AUC_Mean\"].mean(), sub[\"AUC_Std\"].mean())\n",
    "\n",
    "# ===== LOAD & PREP =====\n",
    "all_df = []\n",
    "for bench, path in BENCHMARKS.items():\n",
    "    all_df.append(load_summary(path, bench))\n",
    "summary = pd.concat(all_df, ignore_index=True)\n",
    "\n",
    "# ===== BUILD LATEX MANUALLY (for multirow & multipliers) =====\n",
    "lines = []\n",
    "lines.append(\"\\\\begin{table*}[!ht]\")\n",
    "lines.append(\"\\\\centering\")\n",
    "lines.append(f\"\\\\caption{{{CAPTION}}}\")\n",
    "lines.append(f\"\\\\label{{{LABEL}}}\")\n",
    "lines.append(\"\\\\resizebox{\\\\textwidth}{!}{%\")\n",
    "lines.append(\"\\\\begin{tabular}{lcccc}\")\n",
    "lines.append(\"\\\\toprule\")\n",
    "lines.append(\"Benchmark & Attack & TPR@ & TPR@ & AUC \\\\\\\\ & & 0.001\\\\% FPR (\\\\%) & 0.1\\\\% FPR (\\\\%) & (\\\\%) \\\\\\\\ \")\n",
    "lines.append(\"\\\\midrule\")\n",
    "\n",
    "bench_names = list(BENCHMARKS.keys())\n",
    "baseline_name = bench_names[0]\n",
    "\n",
    "# Pre-slice each benchmark's df for faster lookups\n",
    "bench_dfs = {b: summary[summary[\"Benchmark\"] == b].copy() for b in bench_names}\n",
    "baseline_df = bench_dfs[baseline_name]\n",
    "\n",
    "for bi, bench in enumerate(bench_names):\n",
    "    bdf = bench_dfs[bench]\n",
    "\n",
    "    # We need baseline TPRs per attack to compute multipliers for non-baseline benches\n",
    "    base_tpr1 = {}\n",
    "    base_tpr2 = {}\n",
    "    for atk in ATTACK_ORDER:\n",
    "        mu1, _ = pick_single(baseline_df, atk, ALPHA1)\n",
    "        mu2, _ = pick_single(baseline_df, atk, ALPHA2)\n",
    "        base_tpr1[atk] = mu1\n",
    "        base_tpr2[atk] = mu2\n",
    "\n",
    "    # Multirow count equals number of attacks\n",
    "    lines.append(f\"\\\\multirow{{{len(ATTACK_ORDER)}}}{{*}}{{{bench}}} & \",)\n",
    "\n",
    "    for ai, atk in enumerate(ATTACK_ORDER):\n",
    "        # Pull means/stds for this benchmark & attack\n",
    "        tpr1_mu, tpr1_sd = pick_single(bdf, atk, ALPHA1)\n",
    "        tpr2_mu, tpr2_sd = pick_single(bdf, atk, ALPHA2)\n",
    "        auc_mu,  auc_sd  = pick_auc(bdf, atk)\n",
    "\n",
    "        # Format core numbers\n",
    "        tpr1_txt = fmt_mu_sd(tpr1_mu, tpr1_sd)\n",
    "        tpr2_txt = fmt_mu_sd(tpr2_mu, tpr2_sd)\n",
    "        auc_txt  = fmt_mu_sd(auc_mu,  auc_sd)\n",
    "\n",
    "        # Reduction multipliers (vs baseline) for non-baseline benches\n",
    "        if bench != baseline_name:\n",
    "            r1 = safe_ratio(base_tpr1.get(atk), tpr1_mu)\n",
    "            r2 = safe_ratio(base_tpr2.get(atk), tpr2_mu)\n",
    "            tpr1_txt += \" \" + fmt_multiplier(r1)\n",
    "            tpr2_txt += \" \" + fmt_multiplier(r2)\n",
    "\n",
    "        attack_disp = ATTACK_DISPLAY.get(atk, atk)\n",
    "\n",
    "        if ai == 0:\n",
    "            # first row already started with \\multirow{...} & ...\n",
    "            lines[-1] += f\"{attack_disp} & {tpr1_txt} & {tpr2_txt} & {auc_txt} \\\\\\\\\"\n",
    "        else:\n",
    "            lines.append(f\"& {attack_disp} & {tpr1_txt} & {tpr2_txt} & {auc_txt} \\\\\\\\\")\n",
    "    # midrule between benchmarks\n",
    "    if bi < len(bench_names) - 1:\n",
    "        lines.append(\"\\\\midrule\")\n",
    "\n",
    "lines.append(\"\\\\bottomrule\")\n",
    "lines.append(\"\\\\end{tabular}%\")\n",
    "lines.append(\"}\")\n",
    "lines.append(\"\\\\end{table*}\")\n",
    "\n",
    "latex_table = \"\\n\".join(lines)\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a07d2ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Target vs Shadow at target FPR $=0.001\\%$ across benchmarks. FPR$'$ is the achieved false-positive rate.}\n",
      "\\label{tab:merged_target_shadow_0p001}\n",
      "\\begin{tabular}{lccccccc}\n",
      "\\toprule\n",
      "Benchmark & Attack & $\\tau$ & TPR (\\%) & FPR$'$ (\\%) & Precision@$\\pi$=1\\% & Precision@$\\pi$=10\\% & Precision@$\\pi$=50\\% \\\\\n",
      "\\midrule\n",
      "\\multirow{8}{*}{Purchase-100 (baseline)} & LiRA (online) & Target & 0.523 $\\pm$ 0.243 & 0.000 $\\pm$ 0.000 & 100.000 $\\pm$ 0.000 & 100.000 $\\pm$ 0.000 & 100.000 $\\pm$ 0.000 \\\\\n",
      " & LiRA (online) & Shadow & 0.516 $\\pm$ 0.047 & 0.001 $\\pm$ 0.001 & 89.927 $\\pm$ 11.145 & 98.841 $\\pm$ 1.369 & 99.868 $\\pm$ 0.157 \\\\\n",
      " & LiRA (online, fixed var) & Target & 0.180 $\\pm$ 0.110 & 0.000 $\\pm$ 0.000 & 100.000 $\\pm$ 0.000 & 100.000 $\\pm$ 0.000 & 100.000 $\\pm$ 0.000 \\\\\n",
      " & LiRA (online, fixed var) & Shadow & 0.159 $\\pm$ 0.015 & 0.001 $\\pm$ 0.001 & 78.868 $\\pm$ 22.268 & 96.698 $\\pm$ 3.802 & 99.606 $\\pm$ 0.465 \\\\\n",
      " & LiRA (offline) & Target & 0.007 $\\pm$ 0.007 & 0.000 $\\pm$ 0.000 & 100.000 $\\pm$ 0.000 & 100.000 $\\pm$ 0.000 & 100.000 $\\pm$ 0.000 \\\\\n",
      " & LiRA (offline) & Shadow & 0.004 $\\pm$ 0.002 & 0.001 $\\pm$ 0.001 & 55.819 $\\pm$ 48.265 & 66.203 $\\pm$ 37.648 & 87.370 $\\pm$ 16.561 \\\\\n",
      " & LiRA (offline, fixed var) & Target & 0.022 $\\pm$ 0.017 & 0.000 $\\pm$ 0.000 & 100.000 $\\pm$ 0.000 & 100.000 $\\pm$ 0.000 & 100.000 $\\pm$ 0.000 \\\\\n",
      " & LiRA (offline, fixed var) & Shadow & 0.018 $\\pm$ 0.004 & 0.001 $\\pm$ 0.001 & 57.271 $\\pm$ 43.604 & 80.460 $\\pm$ 21.530 & 96.316 $\\pm$ 4.788 \\\\\n",
      "\\midrule\n",
      "\\multirow{8}{*}{Purchase-100 (reg.)} & LiRA (online) & Target & 0.022 $\\pm$ 0.017 & 0.000 $\\pm$ 0.000 & 100.000 $\\pm$ 0.000 & 100.000 $\\pm$ 0.000 & 100.000 $\\pm$ 0.000 \\\\\n",
      " & LiRA (online) & Shadow & 0.019 $\\pm$ 0.005 & 0.001 $\\pm$ 0.001 & 57.730 $\\pm$ 43.458 & 81.139 $\\pm$ 20.467 & 96.628 $\\pm$ 4.012 \\\\\n",
      " & LiRA (online, fixed var) & Target & 0.026 $\\pm$ 0.019 & 0.000 $\\pm$ 0.000 & 100.000 $\\pm$ 0.000 & 100.000 $\\pm$ 0.000 & 100.000 $\\pm$ 0.000 \\\\\n",
      " & LiRA (online, fixed var) & Shadow & 0.022 $\\pm$ 0.005 & 0.001 $\\pm$ 0.001 & 58.458 $\\pm$ 42.417 & 82.877 $\\pm$ 18.737 & 97.074 $\\pm$ 3.571 \\\\\n",
      " & LiRA (offline) & Target & 0.001 $\\pm$ 0.001 & 0.000 $\\pm$ 0.000 & 100.000 $\\pm$ 0.000 & 100.000 $\\pm$ 0.000 & 100.000 $\\pm$ 0.000 \\\\\n",
      " & LiRA (offline) & Shadow & 0.001 $\\pm$ 0.001 & 0.001 $\\pm$ 0.001 & 27.290 $\\pm$ 44.287 & 30.202 $\\pm$ 42.775 & 42.915 $\\pm$ 40.469 \\\\\n",
      " & LiRA (offline, fixed var) & Target & 0.003 $\\pm$ 0.003 & 0.000 $\\pm$ 0.000 & 100.000 $\\pm$ 0.000 & 100.000 $\\pm$ 0.000 & 100.000 $\\pm$ 0.000 \\\\\n",
      " & LiRA (offline, fixed var) & Shadow & 0.002 $\\pm$ 0.001 & 0.001 $\\pm$ 0.001 & 44.215 $\\pm$ 48.771 & 51.946 $\\pm$ 42.692 & 73.994 $\\pm$ 29.038 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------\n",
    "# ONE unified LaTeX table across many benchmarks\n",
    "# - Target vs Shadow @ chosen Target FPR (%)\n",
    "# - \\multirow grouping by benchmark\n",
    "# - Unified columns: Attack, τ, TPR, FPR', Precision@π=...\n",
    "# - Prints LaTeX only (no saving)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# ===== CONFIG =====\n",
    "BENCHMARKS = [\n",
    "    (\"Purchase-100 (baseline)\", \"analysis_results/purchase100/fcn/2025-05-29_1623_drp_0_wd_5e-5/summary_statistics_two_modes.csv\"),\n",
    "    (\"Purchase-100 (reg.)\",     \"analysis_results/purchase100/fcn/2025-06-15_2326_drp_0.5_wd_1e-3/summary_statistics_two_modes.csv\"),\n",
    "    # (\"CIFAR-100 (TL)\",       \"analysis_results/cifar100/efficientnetv2_rw_s/tl/summary_statistics_two_modes.csv\"),\n",
    "]\n",
    "\n",
    "\n",
    "ALPHA_PERCENT = 0.001       # pick one target FPR (%) for this unified table\n",
    "PRIORS_SHOW   = [0.01, 0.10, 0.50]\n",
    "INCLUDE_GLOBAL = False\n",
    "\n",
    "CAPTION = (f\"Target vs Shadow at target FPR $={ALPHA_PERCENT:.3g}\\\\%$ across benchmarks. \"\n",
    "           \"FPR$'$ is the achieved false-positive rate.\")\n",
    "LABEL   = f\"tab:merged_target_shadow_{str(ALPHA_PERCENT).replace('.','p')}\"\n",
    "\n",
    "# friendly attack order\n",
    "ATTACK_ORDER = [\n",
    "    \"LiRA (online)\",\n",
    "    \"LiRA (online, fixed var)\",\n",
    "    \"LiRA (offline)\",\n",
    "    \"LiRA (offline, fixed var)\",\n",
    "    \"Global threshold\",\n",
    "]\n",
    "\n",
    "def fmt_mu_sd(mu, sd, digits=3):\n",
    "    if pd.isna(mu) or pd.isna(sd):\n",
    "        return \"--\"\n",
    "    return f\"{mu:.{digits}f} $\\\\pm$ {sd:.{digits}f}\"\n",
    "\n",
    "def load_one(csv_path: str, bench: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"Benchmark\"] = bench\n",
    "    return df\n",
    "\n",
    "# ---- load and filter all\n",
    "frames = [load_one(p, b) for (b, p) in BENCHMARKS]\n",
    "all_df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "df = all_df[np.isclose(all_df[\"Target FPR (%)\"], ALPHA_PERCENT, atol=1e-6)].copy()\n",
    "if not INCLUDE_GLOBAL:\n",
    "    df = df[df[\"attack\"] != \"Global threshold\"]\n",
    "df = df[df[\"mode\"].isin([\"target\",\"shadow\"])]\n",
    "\n",
    "# ---- build rows grouped by benchmark, then attack, Target->Shadow\n",
    "precision_cols = [f\"Precision@$\\\\pi$={int(p*100)}\\\\%\" for p in PRIORS_SHOW]\n",
    "lines = []\n",
    "lines.append(\"\\\\begin{table}\")\n",
    "lines.append(f\"\\\\caption{{{CAPTION}}}\")\n",
    "lines.append(f\"\\\\label{{{LABEL}}}\")\n",
    "col_fmt = \"l\" + \"c\"*(3 + 1 + len(PRIORS_SHOW))  # Benchmark + (Attack, tau, TPR, FPR', precisions...)\n",
    "header_cols = [\"Benchmark\",\"Attack\",\"$\\\\tau$\",\"TPR (\\\\%)\",\"FPR$'$ (\\\\%)\"] + precision_cols\n",
    "lines.append(f\"\\\\begin{{tabular}}{{{col_fmt}}}\")\n",
    "lines.append(\"\\\\toprule\")\n",
    "lines.append(\" & \".join(header_cols) + \" \\\\\\\\\")\n",
    "lines.append(\"\\\\midrule\")\n",
    "\n",
    "for bench, _ in BENCHMARKS:\n",
    "    sub_bench = df[df[\"Benchmark\"] == bench]\n",
    "    if sub_bench.empty:\n",
    "        continue\n",
    "    # count rows for multirow: 2 modes per attack if both present\n",
    "    row_count = 0\n",
    "    for atk in ATTACK_ORDER:\n",
    "        present_modes = sub_bench[sub_bench[\"attack\"] == atk][\"mode\"].unique()\n",
    "        row_count += len([m for m in [\"target\",\"shadow\"] if m in present_modes])\n",
    "    if row_count == 0:\n",
    "        continue\n",
    "\n",
    "    # start multirow block\n",
    "    first_row_started = False\n",
    "    for atk in ATTACK_ORDER:\n",
    "        for mode in [\"target\",\"shadow\"]:\n",
    "            sub = sub_bench[(sub_bench[\"attack\"] == atk) & (sub_bench[\"mode\"] == mode)]\n",
    "            if sub.empty: \n",
    "                continue\n",
    "            # TPR / FPR' (independent of prior; averaging a no-op)\n",
    "            tpr_mu, tpr_sd   = sub[\"TPR_Mean\"].mean(), sub[\"TPR_Std\"].mean()\n",
    "            fprp_mu, fprp_sd = sub[\"FPR_Achieved_Mean\"].mean(), sub[\"FPR_Achieved_Std\"].mean()\n",
    "            row = []\n",
    "            if not first_row_started:\n",
    "                row.append(f\"\\\\multirow{{{row_count}}}{{*}}{{{bench}}}\")\n",
    "                first_row_started = True\n",
    "            else:\n",
    "                row.append(\"\")  # empty benchmark cell for subsequent rows in block\n",
    "            row += [atk, mode.title(), fmt_mu_sd(tpr_mu, tpr_sd), fmt_mu_sd(fprp_mu, fprp_sd)]\n",
    "            # add precisions for requested priors\n",
    "            for p in PRIORS_SHOW:\n",
    "                subp = sub[np.isclose(sub[\"prior\"], p, atol=1e-12)]\n",
    "                if subp.empty:\n",
    "                    row.append(\"--\")\n",
    "                else:\n",
    "                    mu, sd = subp[\"Precision_Mean\"].iloc[0], subp[\"Precision_Std\"].iloc[0]\n",
    "                    row.append(fmt_mu_sd(mu, sd))\n",
    "            lines.append(\" & \".join(row) + \" \\\\\\\\\")\n",
    "    lines.append(\"\\\\midrule\")\n",
    "\n",
    "# close table\n",
    "# remove trailing midrule if present\n",
    "if lines[-1] == \"\\\\midrule\":\n",
    "    lines[-1] = \"\\\\bottomrule\"\n",
    "else:\n",
    "    lines.append(\"\\\\bottomrule\")\n",
    "lines.append(\"\\\\end{tabular}\")\n",
    "lines.append(\"\\\\end{table}\")\n",
    "\n",
    "print(\"\\n\".join(lines))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lira-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
